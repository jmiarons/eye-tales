# -*- coding: utf-8 -*-
"""Inference_Image_captioning.ipynb
Automatically generated by Colaboratory.
"""
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

import re
import numpy as np
import os
import time
import json
from glob import glob
from PIL import Image
import pickle
import io
import base64
import cStringIO



# Find the maximum length of any caption in our dataset
def calc_max_length(tensor):
    return max(len(t) for t in tensor)


def download_image(url):
	try:
		response = requests.get(url)
		img = Image.open(BytesIO(response.content)).resize((256, 256), Image.LANCZOS).convert('RGB')
                encodingbuffer = cStringIO.StringIO()
                img.save(encodingbuffer, format="JPEG")
                encodedimage = base64.b64encode(encodingbuffer.getvalue())
                return encodedimage
	except Exception as e:
		print('Error {}'.format(e))
		exit(1)


def init():
	# Download caption annotation files
	annotation_folder = '/annotations/'
	if not os.path.exists(os.path.abspath('.') + annotation_folder):
	  annotation_zip = tf.keras.utils.get_file('captions.zip',
						  cache_subdir=os.path.abspath('.'),
						  origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',
						  extract = True)
	  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'
	  os.remove(annotation_zip)

	# Read the json file
	with open(annotation_file, 'r') as f:
	    annotations = json.load(f)

	# Store captions and image names in vectors
	all_captions = []

	for annot in annotations['annotations']:
	    caption = '<start> ' + annot['caption'] + ' <end>'
	    all_captions.append(caption)

	# Shuffle captions and image_names together
	# Set a random state
	train_captions = shuffle(all_captions, random_state=1)

	# Select the first 30000 captions from the shuffled set
	num_examples = 30000
	train_captions = train_captions[:num_examples]


	# Choose the top 10000 words from the vocabulary
	top_k = 10000
	tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,
							  oov_token="<unk>",
							  filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
	tokenizer.fit_on_texts(train_captions)
	train_seqs = tokenizer.texts_to_sequences(train_captions)
	tokenizer.word_index['<pad>'] = 0
	tokenizer.index_word[0] = '<pad>'

	# Create the tokenized vectors
	train_seqs = tokenizer.texts_to_sequences(train_captions)
	# Pad each vector to the max_length of the captions
	# If you do not provide a max_length value, pad_sequences calculates it automatically
	cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')
	# Calculates the max_length, which is used to store the attention weights
	
	# Encoder and Decoder
	encoder = CNN_Encoder(embedding_dim)
	decoder = RNN_Decoder(embedding_dim, units, vocab_size)
	encoder.load_weights('weights/encoder')
 	decoder.load_weights('weights/decoder')
	return encoder, decoder


max_length = calc_max_length(train_seqs)
BATCH_SIZE = 64
BUFFER_SIZE = 1000
embedding_dim = 256
units = 512
top_k = 5000
vocab_size = top_k + 1

# Shape of the vector extracted from InceptionV3 is (64, 2048)
# These two variables represent that vector shape
features_shape = 2048
attention_features_shape = 64

class BahdanauAttention(tf.keras.Model):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def call(self, features, hidden):
    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)

    # hidden shape == (batch_size, hidden_size)
    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)
    hidden_with_time_axis = tf.expand_dims(hidden, 1)

    # score shape == (batch_size, 64, hidden_size)
    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))

    # attention_weights shape == (batch_size, 64, 1)
    # you get 1 at the last axis because you are applying score to self.V
    attention_weights = tf.nn.softmax(self.V(score), axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * features
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

class CNN_Encoder(tf.keras.Model):
    # Since you have already extracted the features and dumped it using pickle
    # This encoder passes those features through a Fully connected layer
    def __init__(self, embedding_dim):
        super(CNN_Encoder, self).__init__()
        # shape after fc == (batch_size, 64, embedding_dim)
        self.fc = tf.keras.layers.Dense(embedding_dim)

    def call(self, x):
        x = self.fc(x)
        x = tf.nn.relu(x)
        return x

class RNN_Decoder(tf.keras.Model):
  def __init__(self, embedding_dim, units, vocab_size):
    super(RNN_Decoder, self).__init__()
    self.units = units

    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc1 = tf.keras.layers.Dense(self.units)
    self.fc2 = tf.keras.layers.Dense(vocab_size)

    self.attention = BahdanauAttention(self.units)

  def call(self, x, features, hidden):
    # defining attention as a separate model
    context_vector, attention_weights = self.attention(features, hidden)

    # x shape after passing through embedding == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # passing the concatenated vector to the GRU
    output, state = self.gru(x)

    # shape == (batch_size, max_length, hidden_size)
    x = self.fc1(output)

    # x shape == (batch_size * max_length, hidden_size)
    x = tf.reshape(x, (-1, x.shape[2]))

    # output shape == (batch_size * max_length, vocab)
    x = self.fc2(x)

    return x, state, attention_weights

  def reset_state(self, batch_size):
    return tf.zeros((batch_size, self.units))


def load_image(img):
    img = tf.image.decode_base64(img)
    img = tf.image.resize(img, (299, 299))
    img = tf.keras.applications.inception_v3.preprocess_input(img)
    return img, image_path


def evaluate(image, encoder, decoder):
    # Evaluate

    hidden = decoder.reset_state(batch_size=1)

    temp_input = tf.expand_dims(load_image(image)[0], 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))

    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
    result = []

    for i in range(max_length):
        predictions, hidden, _ = decoder(dec_input, features, hidden)

        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()
        result.append(tokenizer.index_word[predicted_id])

        if tokenizer.index_word[predicted_id] == '<end>':
            return result

        dec_input = tf.expand_dims([predicted_id], 0)

    return result

if __name__ == '__main__':
    url = "https://i.ytimg.com/vi/ianIz4tKoDA/maxresdefault.jpg"
    image = download_image(url)
    print(type(image))
